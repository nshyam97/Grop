var(theoretical)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
install.packages("rjags", repos = "http://cran.us.r-project.org")
library(rjags)
# Record the data we want to use in our inference
y = c(10, 8, 8, 11, 7, 5, 9, 8, 7, 8, 13, 11, 7, 9, 6, 10, 8, 9, 7, 9)
# Create a list of data to use in our Bayesian model
# Useful to include the length of our data vector for use in the for loop
data = list(y = y, n = length(y))
# An initial value for our chain. Optional argument -- good for testing convergence.
# If not included parameter are initialised at their prior means
init = list(lambda = 1)
# Specify our model along with prior distributions
# For loops provide a nice method of writing model specifications concisely
# Here we have the model y ~ N(mu, 1); mu ~ dnorm(0, 10)
# Note - JAGS parameterises the Normal in terms of the precision = 1 / variance
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dgamma(10, 1)
}
"
# Compiles the model and runs ``adapt'' phase used to optimise the algorithm
# Not always necessary to have an adapt phase but no real drawbacks to having it so may as well.
# A warning appears if adaptation isn't complete (only likely in very complex models), if this happens
# increase the value of n.adapt and rerun
# n.chains runs multiple chains in parallel. Necessary for some diagnostics (e.g. DIC, Gelman-Rubin etc)
model = jags.model(textConnection(modelstring), data = data, inits = init, n.chains = 2, n.adapt = 1000)
# Carries out an initial run of the model which is then discarded (removes burn-in)
update(model, n.iter = 1000)
# Runs the MCMC algorithm
# We tell JAGS the variables we want it to monitor in the "variable.names" character vector
# n.iter gives the number of iterations we want the algorithm to run for
# thin gives the degree of thinning e.g. if n.iter = 1000 and thin = 5, only every 5th iteration will
# be recorded (so the final output will have 200 samples)
output = coda.samples(model = model, variable.names = c("lambda"), n.iter = 10000, thin = 1)
# Produces posterior summary statistics for each monitored variable
summary(output)
# Produces a traceplot and posterior density for each monitored variable
# If you only wish to view either traceplots or densities switch the other argument to FALSE
plot(output, trace = TRUE, density = TRUE)
# Calculates HDIs for each monitored variable
HPDinterval(output, prob = 0.95)
theoretical = rgamma(10000, 180, 21)
mean(theoretical)
var(theoretical)
mcmc.matrix = output[[1]]
x = seq(0, 20, length.out = 10000) # A range of values we wish to plot
plot(x, dgamma(x, 10, 1), type = "l", xlim = c(0, 20), ylim = c(0, 0.8),
xlab = expression(lambda), ylab = "Density") # Plot the prior
lines(density(mcmc.matrix), col = "red") # Add the posterior density
sum(mcmc.matrix > 10) / length(mcmc.matrix)
install.packages("rjags", repos = "http://cran.us.r-project.org")
install.packages("rjags", repos = "http://cran.us.r-project.org")
install.packages("rjags", repos = "http://cran.us.r-project.org")
knitr::opts_chunk$set(echo = TRUE)
install.packages("rjags", repos = "http://cran.us.r-project.org")
library(rjags)
sum(mcmc.matrix > 10) / length(mcmc.matrix)
pgamma(10, 10, 1)
install.packages("rjags", repos = "http://cran.us.r-project.org")
sum(mcmc.matrix > 10) / length(mcmc.matrix)
1 - pgamma(10, 10, 1)
sum(mcmc.matrix > 10) / length(mcmc.matrix)
# We can use JAGS to form prior and posterior predictive intervals
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dgamma(10, 1)
pred.y ~ dpois(lambda)
prior.lambda ~ dgamma(10, 1)
prior.pred ~ dpois(prior.lambda)
}
"
model = jags.model(textConnection(modelstring), data = data, n.chains = 1, n.adapt = 1000)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("pred.y", "prior.pred"), n.iter = 10000, thin = 1)
HPDinterval(output)
# And calculate prior and posterior predictive probabilities
# E.g. the probability that the next data point will be greater than 3
mcmc.matrix = output[[1]]
sum(mcmc.matrix[ , 1] > 10) / nrow(mcmc.matrix)
sum(mcmc.matrix[ , 2] > 10) / nrow(mcmc.matrix)
# We can use JAGS to form prior and posterior predictive intervals
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dgamma(10, 1)
pred.y ~ dpois(lambda)
prior.lambda ~ dgamma(10, 1)
prior.pred ~ dpois(prior.lambda)
}
"
model = jags.model(textConnection(modelstring), data = data, n.chains = 1, n.adapt = 1000)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("pred.y", "prior.pred"), n.iter = 10000, thin = 1)
# And calculate prior and posterior predictive probabilities
# E.g. the probability that the next data point will be greater than 3
mcmc.matrix = output[[1]]
sum(mcmc.matrix[ , 1] > 10) / nrow(mcmc.matrix)
sum(mcmc.matrix[ , 2] > 10) / nrow(mcmc.matrix)
# We can use JAGS to form prior and posterior predictive intervals
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dgamma(10, 1)
pred.y ~ dpois(lambda)
prior.lambda ~ dgamma(10, 1)
prior.pred ~ dpois(prior.lambda)
}
"
model = jags.model(textConnection(modelstring), data = data, n.chains = 1, n.adapt = 1000)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("pred.y", "prior.pred"), n.iter = 10000, thin = 1)
# And calculate prior and posterior predictive probabilities
# E.g. the probability that the next data point will be greater than 3
mcmc.matrix = output[[1]]
"prior" sum(mcmc.matrix[ , 1] > 10) / nrow(mcmc.matrix)
# We can use JAGS to form prior and posterior predictive intervals
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dgamma(10, 1)
pred.y ~ dpois(lambda)
prior.lambda ~ dgamma(10, 1)
prior.pred ~ dpois(prior.lambda)
}
"
model = jags.model(textConnection(modelstring), data = data, n.chains = 1, n.adapt = 1000)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("pred.y", "prior.pred"), n.iter = 10000, thin = 1)
# And calculate prior and posterior predictive probabilities
# E.g. the probability that the next data point will be greater than 3
mcmc.matrix = output[[1]]
"prior" + sum(mcmc.matrix[ , 1] > 10) / nrow(mcmc.matrix)
# We can use JAGS to form prior and posterior predictive intervals
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dgamma(10, 1)
pred.y ~ dpois(lambda)
prior.lambda ~ dgamma(10, 1)
prior.pred ~ dpois(prior.lambda)
}
"
model = jags.model(textConnection(modelstring), data = data, n.chains = 1, n.adapt = 1000)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("pred.y", "prior.pred"), n.iter = 10000, thin = 1)
# And calculate prior and posterior predictive probabilities
# E.g. the probability that the next data point will be greater than 3
mcmc.matrix = output[[1]]
print("prior: ", sum(mcmc.matrix[ , 1] > 10) / nrow(mcmc.matrix))
sum(mcmc.matrix[ , 2] > 10) / nrow(mcmc.matrix)
# We can use JAGS to form prior and posterior predictive intervals
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dgamma(10, 1)
pred.y ~ dpois(lambda)
prior.lambda ~ dgamma(10, 1)
prior.pred ~ dpois(prior.lambda)
}
"
model = jags.model(textConnection(modelstring), data = data, n.chains = 1, n.adapt = 1000)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("pred.y", "prior.pred"), n.iter = 10000, thin = 1)
# And calculate prior and posterior predictive probabilities
# E.g. the probability that the next data point will be greater than 3
mcmc.matrix = output[[1]]
print("prior: " + sum(mcmc.matrix[ , 1] > 10) / nrow(mcmc.matrix))
# We can use JAGS to form prior and posterior predictive intervals
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dgamma(10, 1)
pred.y ~ dpois(lambda)
prior.lambda ~ dgamma(10, 1)
prior.pred ~ dpois(prior.lambda)
}
"
model = jags.model(textConnection(modelstring), data = data, n.chains = 1, n.adapt = 1000)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("pred.y", "prior.pred"), n.iter = 10000, thin = 1)
# And calculate prior and posterior predictive probabilities
# E.g. the probability that the next data point will be greater than 3
mcmc.matrix = output[[1]]
print("prior: " sum(mcmc.matrix[ , 1] > 10) / nrow(mcmc.matrix))
# We can use JAGS to form prior and posterior predictive intervals
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dgamma(10, 1)
pred.y ~ dpois(lambda)
prior.lambda ~ dgamma(10, 1)
prior.pred ~ dpois(prior.lambda)
}
"
model = jags.model(textConnection(modelstring), data = data, n.chains = 1, n.adapt = 1000)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("pred.y", "prior.pred"), n.iter = 10000, thin = 1)
# And calculate prior and posterior predictive probabilities
# E.g. the probability that the next data point will be greater than 3
mcmc.matrix = output[[1]]
print("prior: ", sum(mcmc.matrix[ , 1] > 10) / nrow(mcmc.matrix))
sum(mcmc.matrix[ , 2] > 10) / nrow(mcmc.matrix)
# We can use JAGS to form prior and posterior predictive intervals
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dgamma(10, 1)
pred.y ~ dpois(lambda)
prior.lambda ~ dgamma(10, 1)
prior.pred ~ dpois(prior.lambda)
}
"
model = jags.model(textConnection(modelstring), data = data, n.chains = 1, n.adapt = 1000)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("pred.y", "prior.pred"), n.iter = 10000, thin = 1)
# And calculate prior and posterior predictive probabilities
# E.g. the probability that the next data point will be greater than 3
mcmc.matrix = output[[1]]
paste("prior: ", sum(mcmc.matrix[ , 1] > 10) / nrow(mcmc.matrix))
sum(mcmc.matrix[ , 2] > 10) / nrow(mcmc.matrix)
# We can use JAGS to form prior and posterior predictive intervals
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dgamma(10, 1)
pred.y ~ dpois(lambda)
prior.lambda ~ dgamma(10, 1)
prior.pred ~ dpois(prior.lambda)
}
"
model = jags.model(textConnection(modelstring), data = data, n.chains = 1, n.adapt = 1000)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("pred.y", "prior.pred"), n.iter = 10000, thin = 1)
# And calculate prior and posterior predictive probabilities
# E.g. the probability that the next data point will be greater than 3
mcmc.matrix = output[[1]]
paste("prior: ", sum(mcmc.matrix[ , 1] > 10) / nrow(mcmc.matrix))
paste("posterior: ", sum(mcmc.matrix[ , 2] > 10) / nrow(mcmc.matrix))
# We can use JAGS to form prior and posterior predictive intervals
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dgamma(10, 1)
pred.y ~ dpois(lambda)
prior.lambda ~ dgamma(10, 1)
prior.pred ~ dpois(prior.lambda)
}
"
model = jags.model(textConnection(modelstring), data = data, n.chains = 1, n.adapt = 1000)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("pred.y", "prior.pred"), n.iter = 10000, thin = 1)
# And calculate prior and posterior predictive probabilities
# E.g. the probability that the next data point will be greater than 3
mcmc.matrix = output[[1]]
print(mcmc.matrix)
paste("prior: ", sum(mcmc.matrix[ , 1] > 10) / nrow(mcmc.matrix))
paste("posterior: ", sum(mcmc.matrix[ , 2] > 10) / nrow(mcmc.matrix))
# We can use JAGS to form prior and posterior predictive intervals
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dgamma(10, 1)
pred.y ~ dpois(lambda)
prior.lambda ~ dgamma(10, 1)
prior.pred ~ dpois(prior.lambda)
}
"
model = jags.model(textConnection(modelstring), data = data, n.chains = 1, n.adapt = 1000)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("pred.y", "prior.pred"), n.iter = 10000, thin = 1)
# And calculate prior and posterior predictive probabilities
# E.g. the probability that the next data point will be greater than 3
mcmc.matrix = output[[1]]
print(mcmc.matrix[,1])
paste("prior: ", sum(mcmc.matrix[ , 1] > 10) / nrow(mcmc.matrix))
paste("posterior: ", sum(mcmc.matrix[ , 2] > 10) / nrow(mcmc.matrix))
# We can use JAGS to form prior and posterior predictive intervals
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dgamma(10, 1)
pred.y ~ dpois(lambda)
prior.lambda ~ dgamma(10, 1)
prior.pred ~ dpois(prior.lambda)
}
"
model = jags.model(textConnection(modelstring), data = data, n.chains = 1, n.adapt = 1000)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("pred.y", "prior.pred"), n.iter = 10000, thin = 1)
# And calculate prior and posterior predictive probabilities
# E.g. the probability that the next data point will be greater than 3
mcmc.matrix = output[[1]]
print(mcmc.matrix)
paste("prior: ", sum(mcmc.matrix[ , 1] > 10) / nrow(mcmc.matrix))
paste("posterior: ", sum(mcmc.matrix[ , 2] > 10) / nrow(mcmc.matrix))
# We can use JAGS to form prior and posterior predictive intervals
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dgamma(10, 1)
pred.y ~ dpois(lambda)
prior.lambda ~ dgamma(10, 1)
prior.pred ~ dpois(prior.lambda)
}
"
model = jags.model(textConnection(modelstring), data = data, n.chains = 1, n.adapt = 1000)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("pred.y", "prior.pred"), n.iter = 10000, thin = 1)
# And calculate prior and posterior predictive probabilities
# E.g. the probability that the next data point will be greater than 3
mcmc.matrix = output[[1]]
print(mcmc.matrix)
paste("posterior: ", sum(mcmc.matrix[ , 1] > 10) / nrow(mcmc.matrix))
paste("prior: ", sum(mcmc.matrix[ , 2] > 10) / nrow(mcmc.matrix))
# We can use JAGS to form prior and posterior predictive intervals
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dgamma(10, 1)
pred.y ~ dpois(lambda)
prior.lambda ~ dgamma(10, 1)
prior.pred ~ dpois(prior.lambda)
}
"
model = jags.model(textConnection(modelstring), data = data, n.chains = 1, n.adapt = 1000)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("pred.y", "prior.pred"), n.iter = 10000, thin = 1)
# And calculate prior and posterior predictive probabilities
# E.g. the probability that the next data point will be greater than 3
mcmc.matrix = output[[1]]
paste("posterior: ", sum(mcmc.matrix[ , 1] > 10) / nrow(mcmc.matrix))
paste("prior: ", sum(mcmc.matrix[ , 2] > 10) / nrow(mcmc.matrix))
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dunif(0, 20)
}
"
model = jags.model(textConnection(modelstring), data = data, inits = init, n.chains = 2, n.adapt = 1000)
# Carries out an initial run of the model which is then discarded (removes burn-in)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("lambda"), n.iter = 10000, thin = 1)
# Produces posterior summary statistics for each monitored variable
summary(output)
plot(output, trace = TRUE, density = TRUE)
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dunif(0, 20)
}
"
model = jags.model(textConnection(modelstring), data = data, inits = init, n.chains = 2, n.adapt = 1000)
# Carries out an initial run of the model which is then discarded (removes burn-in)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("lambda"), n.iter = 10000, thin = 1)
# Produces posterior summary statistics for each monitored variable
summary(output)
mcmc.matrix = output[[1]]
x = seq(0, 20, length.out = 10000) # A range of values we wish to plot
plot(x, dunif(x, 0, 20), type = "l", xlim = c(0, 20), ylim = c(0, 0.8),
xlab = expression(lambda), ylab = "Density") # Plot the prior
lines(density(mcmc.matrix), col = "red") # Add the posterior density
modelstring = "
model {
for(i in 1 : n){
y[i] ~ dpois(lambda)
}
lambda ~ dunif(0, 20)
}
"
model = jags.model(textConnection(modelstring), data = data, inits = init, n.chains = 2, n.adapt = 1000)
# Carries out an initial run of the model which is then discarded (removes burn-in)
update(model, n.iter = 1000)
output = coda.samples(model = model, variable.names = c("lambda"), n.iter = 10000, thin = 1)
mcmc.matrix = output[[1]]
x = seq(0, 20, length.out = 10000) # A range of values we wish to plot
plot(x, dunif(x, 0, 20), type = "l", xlim = c(0, 20), ylim = c(0, 0.8),
xlab = expression(lambda), ylab = "Density") # Plot the prior
lines(density(mcmc.matrix), col = "red") # Add the posterior density
install.packages(c("condMVNorm", "faraway", "ggthemes"))
knitr::opts_chunk$set(echo = TRUE)
install.packages("rjags", repos = "http://cran.us.r-project.org")
library(rjags)
library(knitr)
library(condMVNorm)
modelstring_unemployment = "
model{
y ~ dmnorm.vcov(mu, Sigma)
for(i in 1:n){
mu[i] = mean1950 + trend*(x[i]-1950) + trend2*(u[i] - 5)
}
mean1950 ~ dnorm(0, 1E-6)
trend ~ dnorm(0, 1E-6)
trend2 ~ dnorm(0, 1E-6)
sigsq = pow(sig, 2)
lensq = pow(len, 2)
deltasq = pow(delta, 2)
for(i in 1:n){
Sigma[i,i] = sigsq + deltasq
for(j in (i+1):n){
Sigma[i,j] = sigsq * exp(-0.5 *
pow(x[i] - x[j], 2) / lensq)
Sigma[j,i] = Sigma[i,j]
}
}
sig ~ dt(0, 1, 1) T(0,)
delta ~ dt(0, 1, 1) T(0,)
len ~ dexp(1/5)
}
"
data = list(y = divusa_thinned$marriage,
x = divusa_thinned$year, u=divusa_thinned$unemployed,
n = length(divusa_thinned$year))
library(faraway)
data("divusa")
head(divusa)
divusa_thinned = divusa[seq(1,77,by=3),]
data = list(y = divusa_thinned$marriage,
x = divusa_thinned$year, u=divusa_thinned$unemployed,
n = length(divusa_thinned$year))
model2 = jags.model(textConnection(modelstring_unemployment), data=data,
n.chains = 4, n.adapt = 1000)
update(model, n.iter=2E4)
data = list(y = divusa_thinned$marriage,
x = divusa_thinned$year, u=divusa_thinned$unemployed,
n = length(divusa_thinned$year))
model2 = jags.model(textConnection(modelstring_unemployment), data=data,
n.chains = 4, n.adapt = 1000)
update(model2, n.iter=2E4)
samples2 = coda.samples(model=model2,
variable.names=c("mean1950", "trend", "sig", "delta", "len", "trend2"),
n.iter=3E4, thin=20)
kable(summary(samples2)[[1]][,1:2])
dic2 = dic.samples(model2,n.iter=3E4, thin=20)
print(dic2)
setwd("~/Documents/Group Project/Group-Project-CSC8633")
A_df <- A_df[,-1]
# Import Libraries
library(tidyverse)
library(lubridate)   # work with dates
mydir = "Stack_1" # Change this to direct to your data directory
# List files
A_K2 = list.files(path = mydir, pattern = "*A_K2.csv", full.names = TRUE)
B_K2 = list.files(path = mydir, pattern = "*B_K2.csv", full.names = TRUE)
Power = list.files(path = mydir, pattern = "*POWER.csv", full.names = TRUE)
# Read csv
A_df = A_K2 %>% ldply(read.csv) %>% unique() %>% mutate(Time = as.POSIXct(Time, format="%d/%m/%Y %H:%M:%S"))
library(plyr)
library(lubridate)   # work with dates
mydir = "Stack_1" # Change this to direct to your data directory
# List files
A_K2 = list.files(path = mydir, pattern = "*A_K2.csv", full.names = TRUE)
B_K2 = list.files(path = mydir, pattern = "*B_K2.csv", full.names = TRUE)
Power = list.files(path = mydir, pattern = "*POWER.csv", full.names = TRUE)
# Read csv
A_df = A_K2 %>% ldply(read.csv) %>% unique() %>% mutate(Time = as.POSIXct(Time, format="%d/%m/%Y %H:%M:%S"))
B_df = B_K2 %>% ldply(read.csv) %>% unique() %>% mutate(Time = as.POSIXct(Time, format="%d/%m/%Y %H:%M:%S"))
Power_df = Power %>% ldply(read.csv) %>% unique() %>% mutate(Time = as.POSIXct(Time, format="%d/%m/%Y %H:%M:%S"))
A_df <- A_df[,-1]
B_df <- B_df[,-1]
Power_df <- Power_df[,-1]
# Save combine data as new csv
write.csv(A_df, "A.csv", row.names = FALSE)
write.csv(B_df, "B.csv", row.names = FALSE)
write.csv(Power_df, "Pow.csv", row.names = FALSE)
# Import Libraries
library(plyr)
library(lubridate)   # work with dates
mydir = "Stack_1" # Change this to direct to your data directory
# List files
A_K2 = list.files(path = mydir, pattern = "*A_K2.csv", full.names = TRUE)
B_K2 = list.files(path = mydir, pattern = "*B_K2.csv", full.names = TRUE)
Power = list.files(path = mydir, pattern = "*POWER.csv", full.names = TRUE)
